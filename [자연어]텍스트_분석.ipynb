{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[자연어]텍스트_분석.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOFTyEG4N+q7qTNB9S8Ieo3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ovozzx/Baekjoon/blob/main/%5B%EC%9E%90%EC%97%B0%EC%96%B4%5D%ED%85%8D%EC%8A%A4%ED%8A%B8_%EB%B6%84%EC%84%9D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) NLP \n",
        "→ 기계 번역, 자동 질의응답 시스템\n",
        "\n",
        "## 2) 텍스트 분석\n",
        "→ 텍스트 분류, 감성 분석, 텍스트 요약, 텍스트 군집화\n",
        "\n",
        "## 3) 텍스트 → 피처 벡터화(피처, 빈도 수) : BOW, Word2Vec\n",
        "\n",
        "## 4) 텍스트 분석 프로세스 \n",
        "(1) 텍스트 전처리(클렌징, 대소문자, 토큰화, Stop word, 어근 추출 등) → (2) 피처 벡터화/추출 → (3) 머신러닝 모델링/학습/예측/평가\n"
      ],
      "metadata": {
        "id": "Rd89i7DZF2Qm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "자연어 처리 학습 절차\n",
        "\n",
        "텍스트 전처리 : 정제, 불용어 제거, 어간 추출, 토큰화\n",
        "\n",
        "네트워크 모델 설계\n",
        "\n",
        "모델 훈련\n",
        "\n",
        "모델 예측 및 평가\n",
        "\n"
      ],
      "metadata": {
        "id": "QzkC8iH4uhn2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 텍스트 전처리\n",
        "---\n",
        "\n",
        "-  클렌징 : 불필요한 문자, 기호 제거 ex) HTML, XML, 특정 기호 등\n",
        "- 토큰화 : 문장 or 단어 분리\n",
        "- 스톱 워드(의미 없는 단어) 제거\n",
        "- Stemming & Lemmatization\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "svx4Az6oH_lf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1) 토큰화"
      ],
      "metadata": {
        "id": "bwiTcMqBdAx_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gCcVI0uFyut",
        "outputId": "4843867e-d55a-44d0-e62c-2f380bdc9015"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "<class 'list'> 3\n",
            "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
          ]
        }
      ],
      "source": [
        "# 문장 토큰화 : 마침표, 개행문자 등으로 분리\n",
        "\n",
        "from nltk import sent_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "text_sample = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
        "               You can see it out your window or on your television. \\\n",
        "               You feel it when you go to work, or go to church or pay your taxes.'\n",
        "sentences = sent_tokenize(text=text_sample)\n",
        "print(type(sentences),len(sentences))\n",
        "print(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 토큰화 : 공백, 콤마, 마침표, 개행문자 등으로 분리\n",
        "# Bag of Word와 같이 단어의 순서가 중요하지 않은 경우\n",
        "\n",
        "\n",
        "\n",
        "from nltk import word_tokenize\n",
        "\n",
        "sentence = \"The Matrix is everywhere its all around us, here even in this room.\"\n",
        "words = word_tokenize(sentence)\n",
        "print(type(words), len(words))\n",
        "print(words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVq4K9dCKIWK",
        "outputId": "e6edc39d-2e92-40cf-b9a6-20a94fd2d724"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'> 15\n",
            "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize, sent_tokenize\n",
        "\n",
        "#여러개의 문장으로 된 입력 데이터를 문장별로 단어 토큰화 만드는 함수 생성\n",
        "def tokenize_text(text):\n",
        "    \n",
        "    # 문장별로 분리 토큰\n",
        "    sentences = sent_tokenize(text)\n",
        "    # 분리된 문장별 단어 토큰화\n",
        "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
        "    return word_tokens\n",
        "\n",
        "#여러 문장들에 대해 문장별 단어 토큰화 수행. \n",
        "word_tokens = tokenize_text(text_sample)\n",
        "print(type(word_tokens),len(word_tokens))\n",
        "print(word_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pk2N3JOml3VS",
        "outputId": "4638b08e-64a4-4b33-f57c-fb603e869cef"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'> 3\n",
            "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(2) 스톱 워드 제거 \n",
        "\n",
        "is, the, a, will 등과 같은 문맥적으로 큰 의미가 없는 단어들 제거"
      ],
      "metadata": {
        "id": "SDLPcHfDdC1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2R7Ww_PKlw9",
        "outputId": "80b12143-8269-46dd-e04f-509b4eb59243"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#언어별로 스톱 워드가 목록화\n",
        "print('영어 stop words 갯수:',len(nltk.corpus.stopwords.words('english')))\n",
        "print(nltk.corpus.stopwords.words('english')[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOKXOVgTdoz6",
        "outputId": "e9b8ee7a-e9ef-441f-ea0c-5344e7c4c8df"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "영어 stop words 갯수: 179\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "all_tokens = []\n",
        "# 위 예제의 3개의 문장별로 얻은 word_tokens list 에 대해 stop word 제거 Loop\n",
        "for sentence in word_tokens:\n",
        "    filtered_words=[]\n",
        "    # 개별 문장별로 tokenize된 sentence list에 대해 stop word 제거 Loop\n",
        "    for word in sentence:\n",
        "        #소문자로 모두 변환합니다. \n",
        "        word = word.lower()\n",
        "        # tokenize 된 개별 word가 stop words 들의 단어에 포함되지 않으면 word_tokens에 추가\n",
        "        if word not in stopwords:\n",
        "            filtered_words.append(word)\n",
        "    all_tokens.append(filtered_words)\n",
        "    \n",
        "print(all_tokens) #is, this와 같은 스톱 워드 필터링 통해 제거됨!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K67IQoPEdrNx",
        "outputId": "508c8ec6-f116-4e32-b7e8-6a4220cadbe9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(3) Stemming과 Lemmatization"
      ],
      "metadata": {
        "id": "pQgGdwS1_orX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "\n",
        "print(stemmer.stem('working'),stemmer.stem('works'),stemmer.stem('worked'))\n",
        "print(stemmer.stem('amusing'),stemmer.stem('amuses'),stemmer.stem('amused'))\n",
        "print(stemmer.stem('happier'),stemmer.stem('happiest'))\n",
        "print(stemmer.stem('fancier'),stemmer.stem('fanciest'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKvhpWkS_p06",
        "outputId": "cee36ff9-d9df-4452-da18-3690d1688bd6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "work work work\n",
            "amus amus amus\n",
            "happy happiest\n",
            "fant fanciest\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemma = WordNetLemmatizer()\n",
        "print(lemma.lemmatize('amusing','v'),lemma.lemmatize('amuses','v'),lemma.lemmatize('amused','v'))\n",
        "print(lemma.lemmatize('happier','a'),lemma.lemmatize('happiest','a'))\n",
        "print(lemma.lemmatize('fancier','a'),lemma.lemmatize('fanciest','a'))"
      ],
      "metadata": {
        "id": "TQNoexPc_uPn",
        "outputId": "e378cd5b-6cae-4719-b3b4-511e3f16cf2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "amuse amuse amuse\n",
            "happy happy\n",
            "fancy fancy\n"
          ]
        }
      ]
    }
  ]
}